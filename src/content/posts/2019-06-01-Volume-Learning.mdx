---
title: "Volume Learning"
category: "Learning"
date: "06/01/2019"
published: "false"
slug: "why-we-learn-best-through-sheer-volume"
tags:
    - learning
    - practice
    - the brain
---

Ask any good programmer how to get better, and they'll say the same thing.

Write a *lot* of code.

They never back it up with any reasoning though. We just all know it's true, and so we assume other people do as well. What I'd like to put forth in this post is my intuition for how volume learning improves our ability to understand something.

## Why it works

Early on we improve rapidly because our brain makes connections between things we didn't have a grasp of before, but after a while our improvement slows.

The reason why sheer volume helps us learn things well is because we often don't have the context necessary to digest new content. In his great lecture series on the learning brain, Professor _____XXXXX_______ shows a simple, yet powerful example of how context is essential to storing, remembering, and accessing something you hear for the first time.

He then backs it up with scientific evidence, because guess what? It's all based on a study. Here's how it goes.

____XXXX____ read the passage, explain what it was, look at it again and discuss the results of the published study.

Without context, we don't retain anything we learn. Each time we write more code, view another tutorial, we've established a stronger context for the content we're learning. This makes it easier to consume and process new information - allowing us to relate ideas to our previous knowledge more effectively.

## How I do volume learning

Often when I'm learning a new framework or technology, I read the documentation and understand very little. So I go straight to the examples and try to learn the content through building things. When I come back to the documentation months later, the explanations are crystal clear.
Building a product with the tool in question teaches you just enough to understand written explanations of it.

This works. When I attempted to learn probability, I took a course from MIT and understood about 60% of it. Afterwards, I took the Harvard statistics course and understood a lot more. It's boring to take the same class twice. It's less boring to learn the same thing from two different interesting instructors. Also, every professor has a unique set of lessons that they consider to be the best vehicle for content delivery. 

I'm in the middle of the UCSD probability and statistics course using python and I feel so much more confident about my comprehension of the course materials this time around than I did in the previous two. Take classes, build things, assess yourself, repeat. Do this until you feel like you've truly mastered the content. You'll know because the coursework comes easy for you. As soon as you're ready, move on. 

## What have I mastered with the volume learning approach? 

I've always found this to be the most effective system for learning anything. When I started my data science journey, I poured over every Jupyter notebook and tutorial I could find. I understood some things, but most concepts zipped in one ear and out the other.

Honestly, that's fine.

Most of my time was spent on Kaggle and doing neural network courses (including Fast.ai and Francois Chollet's Deep Learning with Python notebooks which I highly recommend). I made cool stuff, but I never fully grasped how things worked. For me to feel like I know something, I need to have internalized a formal process that I can use as soon as I come across a problem. If I don't have that, I know I'm not ready.

This is why I've found teaching something to be such an effective way to learn things. You have to codify your existing knowledge into digestible chunks so that students can process it themselves.

---

After a few weeks of this, I dove into a more structured environment. Starting with the simplest course is always best - the least amount of context is required.

For me, that was Berkeley's EdX Intro to Data Science Course. They abstract all the numpy, pandas, and matplotlib into their own library. This makes it easy to focus on exactly what it is we need to understand to actually do the things data scientists do. Then I took their Inferential Thinking by Resampling course and the statistics was over my head. Still good though, and definitely worth it.

Afterwards I tried out the DataCamp Python for Data Science series and soared through most of it. I'm pretty meh about DataCamp. Too structured, not enough fun.
The visualization courses were very helpful though. I also side-tracked and did Mode Analytics' intro to pandas and SQL courses. They're great and I highly recommend them.

Next I jumped back over to Kaggle and saw how experts had implemented the things I'd learned in my previous courses. With slightly more bearings than before, I went to a definitive guide - Jake VanderPlas' Data Science for Python Handbook. Hands down the best resource on the subject. Accessible, yet rigorous. He doesn't shy away from complicated code or computer science. At the same time, he also ensures that he covers a wide swath of content. It's a handbook after all, ya know? The final chapters lost me on a lot of the math, but that's fine.

Then I went back to Harvard's CS 109 course on data science and I was loving it. I finally had enough knowledge to actually internalize the content. After completing that course I worked through the notebooks in a General Assembly data science course and completed the exercises as best I could.

Once more, I returned to Kaggle to try out a few competitions and still sucked. That felt ok. It's still fun to watch the experts do it and try to replicate their processes.

Back to structure. This time, I wanted to hone my skills in a number of areas and so I sought out a few resources. Scikit learn pops up everywhere, so I read the original paper and did all the Andreas Mueller courses and workshops I could find. Loved the whole thing.

My conceptual framework was lacking, so I took Caltech's Learning from Data Course and think it's absolutely amazing as well. Take it! Highly recommend it. I also read through an Introduction to Statistical Learning and got rocked on a lot of it. My R skills were underwhelming to say the least. I didn't get through the whole thing, but I'll come back to it eventually.

At the same time I started reading Hal Daume III's Intro to ML book, which is free, written in pseudocode, and overall pretty great. My approach was to read chapters of the book, attempt to turn the pseudocode into python algorithms, and then work through curriculum of a course that closely mirrored the content. I ended up choosing the Kaggle kernels available at https://mlcourse.ai/. This has to be one of the best, most in-depth course into how machine learning algorithms work and is my current favorite for guided practice of writing your own ML algorithms. The combo of these two was great.

All this was going swell until I got my hands on the curriculum for two different data science bootcamps and went through them systematically. It took me about 5 months to complete them both (one was in R and one was in Python). Afterwards I had clearly identified my weakness - statistics and probability. Never took any of those courses in high school or undergrad, so it was back to school for me.

## A more formal education

At this point I felt pretty good. I could complete most exercises on my own and had even completed a few cool projects that would fit my portfolio well. I wasn't content though. My math was eh, and my statistics was nah.

Realizing I needed some guidance, I enrolled in MIT's MicroMasters in Statistics and Data Science program. This was the first time I actually shelled out cash to learn, but it ended up being great. I probably didn't even have to do it, but all the MIT MOOCs I'd taken previously had been top notch, so I decided to support them.

Boy did I need it too. The probability and statistics courses pushed me to the edge of my mental abilities and I actually ended up dropping one class before taking it again later.

I had mostly done machine learning stuff, so I didn't have any rigorous experience completing statistics problem sets. Good to know I still got it. I'll finish up my last machine learning course in the program this fall, and then I'll complete the capstone project.

After that, I'll be done. Yay to me. I feel way more learned than when I started, and I owe it all to sheer volume.

## Who this system is for

Honestly, this isn't going to work for everyone. Some people aren't content not understand each and every detail of something they're learning for the first time.

If you're not ok with just moving on even when you don't know something, you'll hate this.

However, if you can swallow your pride and just move through things, you'll be fine. I promise. Just put your head down and get to work.


## Some other courses I took along the way and loved

+ Kadenze - Creative Applications of Deep Learning with TensorFlow
+ Google's Machine Learning Crash Course
+ EdX - MIT Probability and Statistics
+ MIT - Computer Science with Python
